{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import importlib\n",
    "\n",
    "sys.path.append('../src/')\n",
    "import oracle\n",
    "import agent_b\n",
    "import agent_c\n",
    "import conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'agent_c' from '/Users/shiyimin/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Projects/llm_lead/interviews/test/../src/agent_c.py'>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(conversation)\n",
    "importlib.reload(agent_b)\n",
    "importlib.reload(agent_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## job: data scirntist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### slight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/questionnaires/data_scientist_slight_q.json\", \"r\") as f:\n",
    "    questionnaire = json.load(f)\n",
    "with open(\"../data/scripts/job/data_scientist_slight.json\", \"r\") as f:\n",
    "    script = json.load(f)\n",
    "with open(\"../config/backbone/backbone_configs.json\", \"r\") as f:\n",
    "    backbone_configs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "itr_num = 10\n",
    "a_params = script[\"Public\"]\n",
    "a_params[\"itr_num\"] = itr_num\n",
    "b_params = {\"script_path\":\"../data/scripts/job/data_scientist_slight.json\",\n",
    "            \"role_a\":\"Interviewer\",\n",
    "            \"role_b\":\"Interviewee\"}\n",
    "c_params = {\"role_a\":\"Interviewer\",\n",
    "            \"role_b\":\"Interviewee\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = conversation.Conversation(a_params, b_params, questionnaire, c_params, backbone_configs, itr_num,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.3333333333333333, 'answer_rate': 0.6666666666666666}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.evaluate_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved: []\n",
      "reason: The retrieved relevant pieces do not contain any information about the interviewee's degree, making it impossible to determine the correct answer from the multiple-choice options.\n",
      "answer: None\n",
      "retrieved: [\n",
      "    \"In my final project for the COMP SCI 540 course, I focused on predicting financial trends using machine learning.\",\n",
      "    \"I applied scalable techniques by utilizing the NBEATS and NHITS models within NeuralForecast to predict the price trend of NVDA stock.\",\n",
      "    \"In my project on predicting financial trends using machine learning, I encountered the task of extracting data from large, complex datasets.\",\n",
      "    \"By integrating exogenous variables, like market trends and economic indicators, I was able to enhance the models' predictive accuracy.\",\n",
      "    \"For instance, when evaluating the performance of time series models like NBEATS and NHITS, I utilized confidence intervals to quantify the uncertainty of the forecasts.\",\n",
      "    \"Furthermore, I paid close attention to significance of error measurements, such as Mean Absolute Error (MAE) and Root Mean Square Error (RMSE), to assess the accuracy of the forecasts.\",\n",
      "    \"I typically assess the data's characteristics, such as its distribution and any potential biases, to choose the most suitable statistical methods.\"\n",
      "]\n",
      "reason: The retrieved relevant pieces indicate that the interviewee has experience with financial market prediction and basic statistical concepts. However, there is no mention of familiarity with modern big data systems like Spark, suggesting this might be the area the interviewee is not familiar with.\n",
      "answer: C\n",
      "retrieved: [\n",
      "    \"In my role during the project on predicting financial trends, I played a significant part in collaborating with engineering teams to implement the models in a production environment.\",\n",
      "    \"While working closely with the engineering teams, I ensured that the systems were designed with scalability in mind, which allowed them to manage increasing data loads without compromising performance.\",\n",
      "    \"This collaborative effort resulted in a system that not only supported robust forecasting capabilities but also maintained efficiency and reliability under varying conditions.\",\n",
      "    \"Drawing from my experience with deploying models, I would first collaborate closely with the engineering teams to understand the existing system architecture and constraints.\",\n",
      "    \"Throughout the process, I would prioritize maintaining open communication with all stakeholders to address any concerns and provide updates on the transition progress.\",\n",
      "    \"By leveraging my technical expertise and collaborative skills, I can ensure that the integration of new forecasting models is both seamless and enhances the overall system's capabilities.\"\n",
      "]\n",
      "reason: The retrieved pieces highlight the interviewee's consistent pattern of collaborating effectively with engineering teams, maintaining open communication, and leveraging collaborative skills to enhance system capabilities, indicating they are a collaborative team player.\n",
      "answer: C\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['None', 'C', 'C']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.agent_c.answer_all(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_path = \"../data/scripts/job/data_scientist_normal.json\"\n",
    "q_path = \"../data/questionnaires/data_scientist_normal_q.json\"\n",
    "config_path = \"../config/backbone/backbone_configs.json\"\n",
    "conv2 = conversation.make_conversation(s_path, q_path, config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterative conversation: 100%|████████████████████████████████████████████████████| 10/10 [00:42<00:00,  4.27s/it]\n"
     ]
    }
   ],
   "source": [
    "conv2.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering questions:  33%|███████████████████                                      | 1/3 [00:01<00:03,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved: []\n",
      "reason: The retrieved relevant pieces do not contain any information about the interviewee’s degree, making it impossible to determine the correct answer from the multiple-choice options.\n",
      "answer: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering questions:  67%|██████████████████████████████████████                   | 2/3 [00:05<00:02,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved: [\n",
      "    \"While my experience in causal inference is more limited, I applied statistical inference using Python libraries like pandas and NumPy to analyze the impact of various factors on the financial trends I was forecasting.\",\n",
      "    \"In my project on predicting financial trends using machine learning, I utilized statistical measures to evaluate and enhance the performance of the forecasting models.\",\n",
      "    \"While my current experience with modern batch processing and streaming systems like Spark and Flink is limited, I recognize their potential in managing large-scale data more effectively.\",\n",
      "    \"My primary hands-on experience comes from academic projects, such as my final project on predicting financial trends using machine learning.\",\n",
      "    \"This project allowed me to apply state-of-the-art models like NBEATS and NHITS within the NeuralForecast framework and gain valuable insights into the challenges of model development and deployment.\"\n",
      "]\n",
      "reason: The retrieved pieces indicate the interviewee's familiarity with machine learning theory and statistical concepts. However, they express limited experience with modern big data systems like Spark, making option C the most fitting choice.\n",
      "answer: C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering questions: 100%|█████████████████████████████████████████████████████████| 3/3 [00:10<00:00,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved: [\n",
      "    \"Moving forward, I am eager to work closely with engineering teams to enhance my skills in implementing scalable solutions.\",\n",
      "    \"I recognize the importance of understanding causal relationships in improving forecasting models and am keen to deepen my expertise in this area through future projects and collaborations.\",\n",
      "    \"I'm eager to expand my experience in a professional setting and collaborate with engineering teams to tackle real-world challenges, which I believe will significantly shape and enhance my approach to developing and deploying forecasting models in the future.\",\n",
      "    \"Ultimately, close collaboration with engineering teams will be key.\",\n",
      "    \"Collaborating with engineering teams is an exciting prospect for me because it offers a chance to address some of the challenges I've faced, such as scalability and model deployment.\",\n",
      "    \"By engaging with engineering teams, I can learn how to leverage advanced data processing frameworks and improve data pipeline efficiency.\",\n",
      "    \"Overall, this partnership can bridge the gap between theoretical model development and practical deployment, enabling me to develop more robust, scalable solutions for forecasting in dynamic environments.\"\n",
      "]\n",
      "reason: The retrieved pieces consistently highlight the interviewee's eagerness to collaborate with engineering teams to improve skills in scalable solutions, suggesting a strong inclination towards teamwork and collaboration.\n",
      "answer: C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': 0.3333333333333333, 'answer_rate': 0.6666666666666666}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2.evaluate_performance(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Interviewer: Could you describe a forecasting model you developed or improved in your project on predicting financial trends using machine learning? What techniques did you use, and how did you ensure scalability?\n",
      " Interviewee: In my project on predicting financial trends using machine learning, I focused on applying NBEATS and NHITS models within the NeuralForecast framework to predict the price trend of NVDA stock. To enhance the model's insights, I integrated exogenous variables like market trends, economic indicators, and industry news. These helped provide a richer context for the forecasts. I used Python libraries such as pandas and NumPy for statistical inference, which streamlined the training, testing, and forecasting processes. The models demonstrated remarkable accuracy, even in volatile markets. For visualization, I utilized the plotly package to effectively communicate data trends.\n",
      "\n",
      "However, ensuring scalability was a challenge due to the computational costs and model complexity, which made real-time forecasting difficult. This is an area I am keen to improve upon in future projects.\n",
      " Interviewer: Can you describe a specific instance where you collaborated with an engineering team to implement a model prototype into a production system? What challenges did you face, and how did you address them?\n",
      " Interviewee: While I have experience developing forecasting models, my direct collaboration with engineering teams for implementing model prototypes into production systems is limited. My project primarily focused on building and fine-tuning models like NBEATS and NHITS to predict financial trends. During this process, I encountered challenges related to computational costs and model complexity, which highlighted areas where close collaboration with engineering teams would be beneficial for addressing scalability and deployment issues. Moving forward, I am eager to work closely with engineering teams to enhance my skills in implementing scalable solutions.\n",
      " Interviewer: Can you elaborate on your experience with time series modeling and causal inference, particularly how you've applied these techniques in forecasting applications?\n",
      " Interviewee: In my project on predicting financial trends using machine learning, I applied time series modeling techniques by using the NBEATS and NHITS models within the NeuralForecast framework. These models are designed specifically for time series forecasting, allowing me to effectively predict the price trends of NVDA stock. I integrated exogenous variables such as market trends, economic indicators, and industry news, which provided valuable context and enriched the insights generated by the models.\n",
      "\n",
      "While my experience in causal inference is more limited, I applied statistical inference using Python libraries like pandas and NumPy to analyze the impact of various factors on the financial trends I was forecasting. This approach helped streamline the training, testing, and forecasting processes, enhancing the model's accuracy, even in volatile market conditions.\n",
      "\n",
      "I recognize the importance of understanding causal relationships in improving forecasting models and am keen to deepen my expertise in this area through future projects and collaborations.\n",
      " Interviewer: Could you discuss any experience you have working with statistical measures, such as confidence intervals or significance of error measurements, particularly in the context of evaluating your forecasting models? How did these measures impact your model development process?\n",
      " Interviewee: In my project on predicting financial trends using machine learning, I utilized statistical measures to evaluate and enhance the performance of the forecasting models. While I'm confident in my background in statistics, familiar with concepts like likelihood and probability distributions, my approach focused on statistical inference using Python libraries such as pandas and NumPy. These tools enabled me to perform essential data analysis and visualize the results effectively.\n",
      "\n",
      "The use of statistical measures like confidence intervals and error significance helped in quantifying the uncertainty associated with the model's predictions and guided the model development process by highlighting areas for improvement. Although my direct application of these specific measures might not have been extensive, they formed an underlying framework that informed the adjustments and fine-tuning of the models to ensure accuracy and reliability in predictions.\n",
      "\n",
      "Going forward, I'm enthusiastic about deepening my expertise in applying these statistical measures more comprehensively to further improve model evaluation and development processes.\n",
      " Interviewer: Can you discuss your experience using scripting languages like Perl or Python in your previous projects, particularly in the context of data extraction and model development? How did these tools aid in handling large and complex datasets?\n",
      " Interviewee: In my projects, I've primarily utilized Python as a scripting language for both data extraction and model development, especially in the context of handling large and complex datasets. Python's robust libraries, such as pandas and NumPy, have been instrumental in facilitating data manipulation and statistical analysis. These tools allowed me to streamline the data preprocessing tasks and efficiently manage large datasets by offering functionalities for data cleaning, transformation, and integration of exogenous variables.\n",
      "\n",
      "In my experience with predicting financial trends using machine learning, Python helped in building and fine-tuning models like NBEATS and NHITS, as it offers excellent support for machine learning frameworks like TensorFlow and PyTorch. Although my direct experience with Perl is limited, Python's versatility and powerful libraries made it my go-to language for implementing and experimenting with various data-driven models. This not only accelerated the development process but also enhanced the accuracy and reliability of the forecasts generated from complex datasets.\n",
      " Interviewer: Can you tell me about any experience you have had working in a machine learning or data scientist role at a large technology company? How did this experience shape your approach to developing and deploying forecasting models?\n",
      " Interviewee: As of now, my experience in a machine learning or data scientist role at a large technology company is limited. My primary hands-on experience comes from academic projects, such as my final project on predicting financial trends using machine learning. This project allowed me to apply state-of-the-art models like NBEATS and NHITS within the NeuralForecast framework and gain valuable insights into the challenges of model development and deployment.\n",
      "\n",
      "Working on this project has taught me the importance of integrating exogenous variables and performing statistical inference to enhance model insights. It also highlighted the need for scalability and effective data handling, which would be crucial in a larger corporate environment. I'm eager to expand my experience in a professional setting and collaborate with engineering teams to tackle real-world challenges, which I believe will significantly shape and enhance my approach to developing and deploying forecasting models in the future.\n",
      " Interviewer: Could you elaborate on how you integrated exogenous variables in your financial trend forecasting project? Specifically, what challenges did you face in selecting and incorporating these variables, and how did they contribute to the model's accuracy and insights?\n",
      " Interviewee: In my financial trend forecasting project, integrating exogenous variables was a crucial step to enhance the model's insights. I incorporated variables such as market trends, economic indicators, and industry news, which provided additional context to the predictions made by the NBEATS and NHITS models within the NeuralForecast framework. These exogenous variables helped in capturing external influences that could impact the price trends of NVDA stock.\n",
      "\n",
      "One of the challenges I faced was the selection of relevant exogenous variables. It was important to identify variables that not only had a potential impact on stock prices but were also available in a timely manner for integration into the forecasting models. I had to carefully evaluate the relevance and availability of these variables and determine how to incorporate them without overwhelming the model with noise.\n",
      "\n",
      "Despite these challenges, the integration of exogenous variables significantly contributed to the model's accuracy and insights. They enriched the forecasts by providing a broader perspective on potential influences, thereby allowing the model to account for more factors than purely historical data. This approach ultimately led to better-informed predictions, even in volatile market conditions.\n",
      " Interviewer: You mentioned the importance of scalability and efficient data handling in your projects. Can you discuss specific strategies or techniques you plan to use in future projects to address scalability challenges, particularly in real-time forecasting?\n",
      " Interviewee: In future projects, addressing scalability challenges, especially in real-time forecasting, will be a priority. One strategy I plan to explore is optimizing the computational efficiency of the models. This can involve simplifying model architectures without compromising accuracy, or experimenting with more efficient algorithms that can handle large datasets more gracefully.\n",
      "\n",
      "I would also consider the integration of more advanced data processing frameworks. While my current experience with modern batch processing and streaming systems like Spark and Flink is limited, I recognize their potential in managing large-scale data more effectively. Engaging with engineering teams to explore these tools could greatly enhance data handling capabilities.\n",
      "\n",
      "Another approach would be focusing on efficient data pipeline design, ensuring that data extraction, transformation, and loading (ETL) processes are streamlined to minimize latency. This could involve leveraging SQL and enhancing my understanding of data warehousing principles to better organize and access data.\n",
      "\n",
      "Ultimately, close collaboration with engineering teams will be key. Their expertise can help refine these strategies and ensure the implementation of scalable solutions that meet the demands of real-time forecasting.\n",
      " Interviewer: You mentioned your eagerness to collaborate with engineering teams in the future. Could you elaborate on how you envision this collaboration enhancing your skills in implementing scalable solutions for forecasting models?\n",
      " Interviewee: Collaborating with engineering teams is an exciting prospect for me because it offers a chance to address some of the challenges I've faced, such as scalability and model deployment. In working closely with engineers, I can gain insights into efficient system design and learn about the technical aspects of deploying models in production environments. This collaboration would also allow me to understand better the operational challenges and constraints that come with implementing forecasting models at scale.\n",
      "\n",
      "By engaging with engineering teams, I can learn how to leverage advanced data processing frameworks and improve data pipeline efficiency. Their expertise in handling large datasets and optimizing computational resources can significantly enhance the scalability of my models. Additionally, such collaboration would provide opportunities to refine my skills in data engineering and system architecture, ensuring that my forecasting models can be effectively integrated into real-time applications.\n",
      "\n",
      "Overall, this partnership can bridge the gap between theoretical model development and practical deployment, enabling me to develop more robust, scalable solutions for forecasting in dynamic environments.\n",
      " Interviewer: Can you discuss a specific example where you utilized Python's capabilities to enhance a model's scalability, particularly focusing on managing computational costs and handling large datasets?\n",
      " Interviewee: In my experience with Python, I've found it incredibly useful for enhancing a model's scalability, particularly when dealing with large datasets. For example, in my financial trend forecasting project, I used Python's robust libraries like pandas and NumPy to streamline data preprocessing tasks. These libraries facilitated efficient data manipulation and statistical analysis, which was crucial in managing large datasets and reducing computational costs.\n",
      "\n",
      "By leveraging Python's capabilities, I was able to perform data cleaning, transformation, and integration of exogenous variables in a more streamlined manner. This helped to ensure that the data fed into the models was both accurate and efficiently processed, which in turn contributed to the models' scalability. Although there were challenges with computational costs due to model complexity, Python provided the necessary tools to handle these tasks more effectively.\n",
      "\n",
      "Moving forward, I am keen to explore more advanced techniques and frameworks that could further enhance scalability, such as optimizing model architectures and collaborating with engineering teams to implement more efficient data handling strategies.\n"
     ]
    }
   ],
   "source": [
    "print(conv2.agent_b.hist_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
